{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Demo - Predicting Face Touches\n",
        "##“Automatic Detection of Face Touch and Self-adaptors in Infants”\n",
        "\n",
        "This library uses some capabilities from Chambers et al. study and their repositories for infant skeleton tracking.\n",
        "\n",
        "- https://github.com/cchamber/Infant_movement_assessment\n",
        "- Chambers, Claire; Seethapathi, Nidhi; Saluja, Rachit; Johnson, Michelle; Kording, Konrad Paul (2019): Computer vision to automatically assess infant neuromotor risk. figshare. Dataset. https://doi.org/10.6084/m9.figshare.8161430.v5 "
      ],
      "metadata": {
        "id": "VqqBnQVrtKp1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLSAYml0h-n1"
      },
      "source": [
        "##<b>Part 0: Download dependencies and library</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94BaA_HaiDd1"
      },
      "source": [
        "###1. Download Git repository, create required folders and mount drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oael1_d8nxDX"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/bt403/openpose-research-keras\n",
        "!mkdir /content/openpose-research-keras/model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeS_M6roiIz1"
      },
      "source": [
        "###2. Download library dependencies for OpenPose"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zP0T-0MnRbA",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title RUN Download dependencies\n",
        "!apt-get install ffmpeg --yes\n",
        "!pip install ConfigObj\n",
        "!pip install sk-video\n",
        "!pip install tqdm\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "!pip install mediapipe\n",
        "!pip install face-alignment\t\n",
        "!pip install arff\n",
        "!pip install scikit-multilearn\n",
        "\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "GPUs = GPU.getGPUs()\n",
        "if (len(GPUs) > 0):\n",
        "  gpu = GPUs[0]\n",
        "  def printm():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "  printm()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fnD6LyIii4A"
      },
      "source": [
        "##<b>Part I: Export frames and pose from videos</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Place videos in sample_videos folder\n",
        "The videos that need to be processed should be placed in the \"sample_videos\" folder. In this case, the videos are copied from Drive. "
      ],
      "metadata": {
        "id": "HAcKzZdm2GgC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT1z-MIDFyEo"
      },
      "outputs": [],
      "source": [
        "%cp /content/drive/MyDrive/ResearchProject/videos-mp4/* /content/openpose-research-keras/sample_videos/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming there are videos inside the folder, the following steps can be run.\n",
        "\n",
        "###2. Execute command to export frames from videos.\n",
        "The following script exports frames from videos for tagging and training. The command exports 3 frames per second. The main parameters are the following:\n",
        "\n",
        "1.   <b>videos_path</b>: Path where the videos are located. By default it loads videos from ./sample_videos .\n",
        "2.   <b>export_images_path</b>: Path where the videos will be saved\n",
        "\n"
      ],
      "metadata": {
        "id": "WTFa3Adc3WSf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1CMC-33FX-Z"
      },
      "outputs": [],
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/export_frames.py --videos_path './sample_videos' --export_images_path '/content/drive/MyDrive/ResearchProject/exported_frames/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqw1yvvZQN6t"
      },
      "source": [
        "###3. Download and copy OpenPose infant model to folder\n",
        "The following script downloads Chambers et al. model from their open repository.\n",
        "\n",
        "- Chambers, Claire; Seethapathi, Nidhi; Saluja, Rachit; Johnson, Michelle; Kording, Konrad Paul (2019): Computer vision to automatically assess infant neuromotor risk. figshare. Dataset. https://doi.org/10.6084/m9.figshare.8161430.v5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1k6gSOBQPNr"
      },
      "outputs": [],
      "source": [
        "%cd /content/\n",
        "!wget https://figshare.com/ndownloader/articles/8161430?private_link=10034c230ad9b2b2a6a4\n",
        "!unzip \"/content/8161430?private_link=10034c230ad9b2b2a6a4\"\n",
        "!unzip pose_extraction.zip\n",
        "%cd /content/openpose-research-keras/\n",
        "%cp -a /content/pose_extraction/colab_openpose/models/model.h5 /content/openpose-research-keras/model/model.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Execute command to export poses from videos.\n",
        "The following script exports the infant pose data from videos. The main parameters are the following:\n",
        "\n",
        "1.   <b>videos_path</b>: Path where the videos are located. By default it loads videos from ./sample_videos\n",
        "2.   <b>model_path</b>: Path where the openpose model is located. By default it loads from ./model/model.h5."
      ],
      "metadata": {
        "id": "CAIlraN-8fUo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiMgawvyMOpe"
      },
      "outputs": [],
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/export_openpose_values.py --videos_path './sample_videos' --model_path './model/model.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Copy exported raw coordinates to data folder."
      ],
      "metadata": {
        "id": "s7TvtQ5oBka2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hkHvLkJWQaw9"
      },
      "outputs": [],
      "source": [
        "%mkdir /content/openpose-research-keras/data\n",
        "%mkdir /content/openpose-research-keras/data/processed\n",
        "%mkdir /content/openpose-research-keras/data/estimates\n",
        "%cp  /content/openpose-research-keras/sample_videos/* /content/openpose-research-keras/data/processed"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Run processing of exported coordinates\n",
        "The following script processes the coordinates of the videos by selecting limbs, smoothing, normalising and interpolating. The main parameters are the following:\n",
        "\n",
        "1.   <b>input_path</b>: Path where the raw coordinates and videos are located. By default it loads the input from ./data/processed/\n",
        "2.   <b>output_path</b>: Path where the new processed estimates are saved. By default it saves them to: ./data/estimates/"
      ],
      "metadata": {
        "id": "-p09K9UVBunL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/process_pose_values.py --input_path './data/processed/' --output_path './data/estimates/'"
      ],
      "metadata": {
        "id": "DY58rLmaEBTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-CG36nXobpR"
      },
      "source": [
        "##<b>Part II: Export aggregated features</b>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Requirements\n",
        "\n",
        "This section requires the following inputs:\n",
        "1. <b>Generated pose estimates</b>: Ensure the output coordinates generated in Part I are located in the ./data/estimates/ coordinates.\n",
        "2. <b>Generated image frames</b>: Path for exported image frames.\n",
        "3. <b>CSV file with tagged data</b>: The CSV file should contain the following columns: \n",
        "- name_image: {video_name}\\_{video_id}\\_{frame_num_in_video}\\_{frame_id}. Example: video_1_1001_121.jpg. These should match the extracted frames.\n",
        "- head_area: on-head or outside-head depending if the infant is touching his face.\n",
        "- ears: 1 or 0. Depending if the infant is touching this location.\n",
        "- neck: 1 or 0. Depending if the infant is touching this location.\t\n",
        "- mouth: 1 or 0. Depending if the infant is touching this location.\n",
        "- cheeks: 1 or 0. Depending if the infant is touching this location.\t\n",
        "- eyes: 1 or 0. Depending if the infant is touching this location.\n",
        "- nose: 1 or 0. Depending if the infant is touching this location.\t\n",
        "- forehead: 1 or 0. Depending if the infant is touching this location.\n",
        "\n",
        "The number of rows of the CSV file should match the number of image frames found in the path where the generated image frames are located. In this case the following script copies the file with the tagged image data from Drive."
      ],
      "metadata": {
        "id": "PhnWdDmXYkie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9aHBPZwTpUxm"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/ResearchProject/videos-processed/data_full_csv.csv /content/openpose-research-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Execute extraction of aggregated features\n",
        "The following command executes the script to export the processed values. This command will generate and save the distance and angular features. It also will extract the face and wrist region of the infants in each frame. The required arguments are the following:\n",
        "1.   <b>exported_frames</b>: Path where the exported frames are located. By default it loads the frames from /content/drive/MyDrive/ResearchProject/exported_frames/\n",
        "2.   <b>cropped_output_path</b>: Path where cropped image frames will be saved. By default it saves the output images to /content/drive/MyDrive/ResearchProject/cropped/\n",
        "3.   <b>csv_data</b>: Path of CSV file described above. By default it loads from /content/openpose-research-keras/data_full_csv.csv\n",
        "4. <b>pose_estimates_path</b>: Path of generated pose estimates. By default it loads from /content/openpose-research-keras/data_full_csv.csv\n",
        "5. <b>output_path</b>: Path to save generated features. By defaults it saves them to /content/openpose-research-keras/data/estimates/processed_features.pkl\n"
      ],
      "metadata": {
        "id": "egzJ-jSkKU_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/process_feature_extraction.py --exported_frames '/content/drive/MyDrive/ResearchProject/exported_frames/'  --cropped_output_path '/content/drive/MyDrive/ResearchProject/cropped3/' --csv_data '/content/openpose-research-keras/data_full_csv_v2.csv'  --pose_estimates '/content/openpose-research-keras/data/estimates/'"
      ],
      "metadata": {
        "id": "gSV8QslWIlVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Execute extraction of smaller face areas and confidence scores\n",
        "After obtaining the aggregated features, the following code will obtain the exported exported mouth area, nose area and their confidence scores using 3D-FAN. This script requires the following parameters as input:\n",
        "\n",
        "1.   <b>processed_features</b>: Path to the exported processed features. By default the features were exported to './data/estimates/processed_features.pkl'.\n",
        "2.   <b>exported_frames</b>: Path where the exported frames are located. By default it loads the frames from /content/drive/MyDrive/ResearchProject/exported_frames/\n",
        "3.   <b>eyes_path</b>: Path where the exported cropped eyes region will be saved. By default it saves the images to: /content/drive/MyDrive/ResearchProject/mouth/\n",
        "4. <b>mouth_path</b>: Path where the exported cropped mouth region will be saved. By default it saves the images to: /content/drive/MyDrive/ResearchProject/mouth/\n",
        "\n",
        "The code will save the updated processed_features in the same path."
      ],
      "metadata": {
        "id": "bdrp2ocOMhkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/process_face_region.py --processed_features './data/estimates/processed_features.pkl' --exported_frames '/content/drive/MyDrive/ResearchProject/exported_frames/' --eyes_path '/content/drive/MyDrive/ResearchProject/eyes/' --mouth_path '/content/drive/MyDrive/ResearchProject/mouth/'"
      ],
      "metadata": {
        "id": "ldIz-PrINU2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Extract HOG Features\n",
        "Based on the obtained images for eyes, mouth and face, the HOG features can be extracted. \n",
        "\n",
        "The following code extracts the HOG features of these regions and saves them. The code receives the following inputs:\n",
        "1.   <b>processed_features</b>: Path to the exported processed features. By default the features were exported to './data/estimates/processed_features.pkl'.\n",
        "2.   <b>output_path</b>: Path where the HOG features will be saved. By default it saves them to ./data/estimates/\n",
        "2.   <b>eyes_folder</b>: Path where the eyes area images were saved. By default it loads them from /content/drive/MyDrive/ResearchProject/eyes/ .\n",
        "2.   <b>mouth_folder</b>: Path where the mouth area images were saved. By default it loads them from /content/drive/MyDrive/ResearchProject/mouth/ ."
      ],
      "metadata": {
        "id": "zH7q2Irs7F4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/extract_hog_features.py"
      ],
      "metadata": {
        "id": "y8qawo8B7Sr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part III: Preprocess and augment features\n",
        "\n",
        "This section performs final preprocessing and augmentation before running the classification algorithms.\n",
        "\n",
        "1.   <b>processed_features</b>: Path to the exported processed features. By default the features were exported to './data/estimates/processed_features.pkl'.\n",
        "2.   <b>final_features</b>: Path where the final features will be saved after preprocessing. By default it saves them to ./data/estimates/final_features.pkl"
      ],
      "metadata": {
        "id": "eCE5tgoqMhgp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/preprocess_features.py "
      ],
      "metadata": {
        "id": "dtz5gHpLqrKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part IV: Train Classifier - RF - PCA - SVM Model\n",
        "This section trains the classifiers based on the final preprocessed data. However, before training the classifier, the data should be divided into training and testing. However this is dependant on the problem so I will show an example based on sample data, but it could vary depending on the context."
      ],
      "metadata": {
        "id": "8Erib15Wqqoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Divide train and test data\n",
        "\n",
        "The following code loads the preprocessed data and divides it into training and testing sets. Then it saves them to a pickle file."
      ],
      "metadata": {
        "id": "Uc3nRszMqqk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score, cross_validate\n",
        "pdata = pd.read_pickle('./data/estimates/final_features.pkl')\n",
        "augmented = True\n",
        "\n",
        "if augmented:\n",
        "  pdata_augmented = pd.read_pickle('./data/estimates/final_features_augmented.pkl')\n",
        "\n",
        "splitter = GroupShuffleSplit(test_size=.5, n_splits=2, random_state = 1)\n",
        "split = splitter.split(pdata, groups=pdata['video_name'])\n",
        "train_inds, test_inds = next(split)\n",
        "train = pdata.iloc[train_inds].copy()\n",
        "test = pdata.iloc[test_inds].copy()\n",
        "\n",
        "if (augmented):\n",
        "  train_augmented = pdata_augmented.iloc[train_inds].copy()\n",
        "  train = pd.concat([train, train_augmented])\n",
        "\n",
        "train.to_pickle('./data/estimates/train_data.pkl')\n",
        "test.to_pickle('./data/estimates/test_data.pkl')"
      ],
      "metadata": {
        "id": "Ou-PcIrntQnm"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Train classifier\n",
        "\n",
        "The following scripts performs Feature Selection with Random Forest, Feature Reduction with PCA and Classification with SVM. Also, it cross-validates with 5 folds and does a GridSearch to look for the best hyperparameters in the training set. The pipeline also standardises the values and fills any remaining missing value based on the training set.\n",
        "\n",
        "The script receives as input the following parameters:\n",
        "1.   <b>train_data</b>: Path to pkl file with train data. By default it loads the file from './data/estimates/train_data.pkl'.\n",
        "2.   <b>test_data</b>: Path to pkl file with test data. By default it loads the file from './data/estimates/test_data.pkl'."
      ],
      "metadata": {
        "id": "RMbJzKqQMhOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/train_classifier_rf_pca_svm.py "
      ],
      "metadata": {
        "id": "jhq53UkJyfO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Part V: Train Classifier - Autoencoder Model"
      ],
      "metadata": {
        "id": "MoCV-T6z93yN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Divide train and test data\n",
        "\n",
        "The following code loads a sample preprocessed data and divides it into training and testing sets. Then it saves them to a pickle file."
      ],
      "metadata": {
        "id": "JjHu4Z9E-LPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import GroupShuffleSplit, GroupKFold, cross_val_score, cross_validate\n",
        "pdata = pd.read_pickle('./data/estimates/final_features.pkl')\n",
        "augmented = True\n",
        "\n",
        "if augmented:\n",
        "  pdata_augmented = pd.read_pickle('./data/estimates/final_features_augmented.pkl')\n",
        "\n",
        "splitter = GroupShuffleSplit(test_size=.5, n_splits=2, random_state = 1)\n",
        "split = splitter.split(pdata, groups=pdata['video_name'])\n",
        "train_inds, test_inds = next(split)\n",
        "train = pdata.iloc[train_inds].copy()\n",
        "test = pdata.iloc[test_inds].copy()\n",
        "\n",
        "if (augmented):\n",
        "  train_augmented = pdata_augmented.iloc[train_inds].copy()\n",
        "  train = pd.concat([train, train_augmented])\n",
        "\n",
        "train.to_pickle('./data/estimates/train_data.pkl')\n",
        "test.to_pickle('./data/estimates/test_data.pkl')\n",
        "\n",
        "hog_features_face = pd.read_pickle('./data/estimates/hog_features_face.pkl')\n",
        "hog_features_eyes = pd.read_pickle('./data/estimates/hog_features_eyes.pkl')\n",
        "hog_features_mouth = pd.read_pickle('./data/estimates/hog_features_mouth.pkl')\n",
        "hog_features_face_flipped = pd.read_pickle('./data/estimates/hog_features_face_flipped.pkl')\n",
        "hog_features_eyes_flipped = pd.read_pickle('./data/estimates/hog_features_eyes_flipped.pkl')\n",
        "hog_features_mouth_flipped = pd.read_pickle('./data/estimates/hog_features_mouth_flipped.pkl')\n",
        "hog_features = pd.concat((hog_features_face, hog_features_eyes, hog_features_mouth), axis=1)\n",
        "hog_features_flipped = pd.concat((hog_features_face, hog_features_eyes, hog_features_mouth), axis=1)\n",
        "\n",
        "hog_features_train = hog_features.iloc[train_inds]\n",
        "hog_features_train_flipped = hog_features_flipped.iloc[train_inds]\n",
        "hog_features_train = pd.concat([hog_features_train, hog_features_train_flipped])\n",
        "hog_features_test = hog_features.iloc[test_inds]\n",
        "\n",
        "hog_features_train.to_pickle('./data/estimates/train_data_hog.pkl')\n",
        "hog_features_test.to_pickle('./data/estimates/test_data_hog.pkl')"
      ],
      "metadata": {
        "id": "Fn1vOWVm-Fmh"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Train classifier\n",
        "\n",
        "The following scripts performs the training for the Autoencoders with SVM classifier. Also, it cross-validates with 5 folds and does a GridSearch to look for the best hyperparameters in the training set. The pipeline also standardises the values and fills any remaining missing value based on the training set.\n",
        "\n",
        "The script receives as input the following parameters:\n",
        "1.   <b>train_data</b>: Path to pkl file with train data. By default it loads the file from './data/estimates/train_data.pkl'.\n",
        "2.   <b>test_data</b>: Path to pkl file with test data. By default it loads the file from './data/estimates/test_data.pkl'.\n",
        "3.   <b>train_data_hog</b>: Path to pkl file with train hog data. By default it loads the file from './data/estimates/train_data_hog.pkl'.\n",
        "4.   <b>test_data_hog</b>: Path to pkl file with test hog data. By default it loads the file from './data/estimates/test_data_hog.pkl'."
      ],
      "metadata": {
        "id": "79p5kjqK-Qz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/openpose-research-keras/\n",
        "!CUDA_VISIBLE_DEVICES=0 python3 /content/openpose-research-keras/train_classifier_autoencoder.py "
      ],
      "metadata": {
        "id": "IdOjlf24-VTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "name": "Demo-Predicting-Face-Touches.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}